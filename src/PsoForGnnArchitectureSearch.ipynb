{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Swarm Optimisation for Graph Neural Network Architecture Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.datasets import MNISTSuperpixels\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from particle import Particle\n",
    "# from swarm import Swarm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This GNN can handle different types of convolutional layers, and both node and graph classification.\n",
    "The build_conv_model method determines which type of convolutional layer to use for the given task, a graph convolutional network for node classificationtion (GCNConv) and a graph isomorphism network for graph classification (GINConv).\n",
    "This model is made of 3 covolution layers followed by mean pooling in the case of graph classification, followed by 2 fully connected layers.\n",
    "Sing our goal here is classification, we use a negative log-likelihood loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of the changeable hyperparameters:\n",
    "\n",
    "    - hidden_dim (dimmension of hidden layers)\n",
    "\n",
    "    - number of convs layers\n",
    "\n",
    "    - number of lns layers (normalisation layers)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNStack(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, task='node', hidden_num = 2, hidden_dim = 32):\n",
    "        super(GNNStack, self).__init__()\n",
    "        self.task = task\n",
    "        self.convs = nn.ModuleList() #convolution operations\n",
    "        self.lns = nn.ModuleList() #normalisation operations\n",
    "        self.convs.append(self.build_conv_model(input_dim, hidden_dim))\n",
    "        # adding convolution an normalisation layers\n",
    "        for l in range(hidden_num):\n",
    "            self.convs.append(self.build_conv_model(hidden_dim, hidden_dim))\n",
    "            self.lns.append(nn.LayerNorm(hidden_dim))\n",
    "\n",
    "        # self.lns.append(nn.LayerNorm(hidden_dim))\n",
    "\n",
    "\n",
    "        # post-message-passing\n",
    "        self.post_mp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.Dropout(0.25), \n",
    "            nn.Linear(hidden_dim, output_dim)) # adding 2 linear layers \n",
    "        if not (self.task == 'node' or self.task == 'graph'):\n",
    "            raise RuntimeError('Unknown task.')\n",
    "\n",
    "        self.dropout = 0.25\n",
    "        self.num_layers = 3\n",
    "\n",
    "    def build_conv_model(self, input_dim, hidden_dim):\n",
    "        # refer to pytorch geometric nn module for different implementation of GNNs.\n",
    "        # create different types of GCNConv according to the problem to solve\n",
    "        if self.task == 'node':\n",
    "            return pyg_nn.GCNConv(input_dim, hidden_dim)\n",
    "        else:\n",
    "            return pyg_nn.GINConv(nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
    "                                  nn.ReLU(), nn.Linear(hidden_dim, hidden_dim)))\n",
    "\n",
    "    def forward(self, data):\n",
    "        # x = feature matrix = number of nodes * number of node feature dimensions,\n",
    "        # edge_index = list of the edges in the graph,\n",
    "        # batch = batch of a graph\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        if data.num_node_features == 0: #if there is no feature, use a constant feature\n",
    "          x = torch.ones(data.num_nodes, 1)\n",
    "\n",
    "        for i in range(self.num_layers): # ,create num_layers convolution layers\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            emb = x\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            if not i == self.num_layers - 1:\n",
    "                x = self.lns[i](x)\n",
    "\n",
    "        if self.task == 'graph': # if it is a graph classification task, do a pooling\n",
    "            x = pyg_nn.global_mean_pool(x, batch)\n",
    "\n",
    "        x = self.post_mp(x)\n",
    "\n",
    "        return emb, F.log_softmax(x, dim=1)\n",
    "\n",
    "    def loss(self, pred, label):\n",
    "        return F.nll_loss(pred, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyg_nn.GCNConv and pyg_nn.GINConv are instances of MessagePassing, They define a single layer of graph convolution, which can be decomposed into:\n",
    "- Message computation\n",
    "- Aggregation\n",
    "- Update\n",
    "- Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With CustomConv, we have an exemple of how to subclass the pytorch geometric MessagePassing class to derive a new model  rather than using existing GCNConv and GINConv.\n",
    "We use the MessagePassing's key building blocks:\n",
    "- aggr='add': The aggregation method to use (\"add\", \"mean\" or \"max\").\n",
    "- propagate(): The initial call to start propagating messages. Takes in the edge indices and any other data to pass along (e.g. to update node embeddings).\n",
    "- message(): Constructs messages to node i. Takes any argument which was initially passed to propagate().\n",
    "- update(): Updates node embeddings. Takes in the output of aggregation as first argument and any argument which was initially passed to propagate()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConv(pyg_nn.MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CustomConv, self).__init__(aggr='add')  # \"Add\" aggr egation.\n",
    "        self.lin = nn.Linear(in_channels, out_channels)\n",
    "        self.lin_self = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Add self-loops to the adjacency matrix.\n",
    "        edge_index, _ = pyg_utils.remove_self_loops(edge_index)\n",
    "\n",
    "        # Transform node feature matrix.\n",
    "        self_x = self.lin_self(x)\n",
    "        #x = self.lin(x)\n",
    "\n",
    "        # start the propagation of the message\n",
    "        return self_x + self.propagate(edge_index, size=(x.size(0), x.size(0)), x=self.lin(x))\n",
    "\n",
    "    def message(self, x_i, x_j, edge_index, size):\n",
    "        # Compute messages\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = pyg_utils.degree(row, size[0], dtype=x_j.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        return x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model with forward propagation and back propagation. \n",
    "For node classification, we split nodes into training and testing sets.\n",
    "Same thing for graph classification, we use 80% of the graphs for training and the remainder for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, task, writer, hidden_num , hidden_dim):\n",
    "    trigger = 0\n",
    "    prev_acc = 0.0\n",
    "\n",
    "    if task == 'graph':\n",
    "        data_size = len(dataset)\n",
    "        # split data  into traning and testing set\n",
    "        loader = DataLoader(dataset[:int(data_size * 0.8)], batch_size=64, shuffle=True)\n",
    "        test_loader = DataLoader(dataset[int(data_size * 0.8):], batch_size=64, shuffle=True)\n",
    "    else:\n",
    "        test_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "        loader = test_loader\n",
    "\n",
    "    # build model\n",
    "    model = GNNStack(max(dataset.num_node_features, 1), dataset.num_classes, task=task, hidden_num=hidden_num, hidden_dim=hidden_dim)\n",
    "    opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # train for 200 epochs\n",
    "    print(\"hidden_num = \" + str(hidden_num))\n",
    "    print(\"hidden_dim = \" + str(hidden_dim))\n",
    "    for epoch in range(200):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in loader:\n",
    "            # print(batch.train_mask, '----')\n",
    "            opt.zero_grad()\n",
    "            # get the prediction and the excpeted label\n",
    "            embedding, pred = model(batch)\n",
    "            # pred = model(batch)\n",
    "            label = batch.y\n",
    "            if task == 'node':\n",
    "                pred = pred[batch.train_mask]\n",
    "                label = label[batch.train_mask]\n",
    "            loss = model.loss(pred, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        total_loss /= len(loader.dataset)\n",
    "        writer.add_scalar(\"loss\", total_loss, epoch)\n",
    "\n",
    "        \n",
    "        test_acc = test(test_loader, model)\n",
    "        # print(\"Epoch {}. Loss: {:.4f}. Test accuracy: {:.4f}\".format(\n",
    "        #     epoch, total_loss, test_acc))\n",
    "        # writer.add_scalar(\"test accuracy\", test_acc, epoch)\n",
    "\n",
    "        if test_acc <= prev_acc:\n",
    "            trigger += 1\n",
    "\n",
    "        if trigger >= 10:\n",
    "            print(\"the model is not learning anymore.\")\n",
    "            print(\"Epoch {}. Loss: {:.4f}. Test accuracy: {:.4f}\".format(\n",
    "            epoch, total_loss, test_acc))\n",
    "            writer.add_scalar(\"test accuracy\", test_acc, epoch)\n",
    "            break\n",
    "        prev_acc = test_acc\n",
    "\n",
    "    return model, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the CiteSeer/Cora node classification task, there is only one graph, so we use masking to determine validation and test set. In graph classification, a subset of graphs is considered as a validation/test graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, model, is_validation=False):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        with torch.no_grad(): # avoid gradient computing for faster results\n",
    "            emb, pred = model(data)\n",
    "            pred = pred.argmax(dim=1)\n",
    "            label = data.y\n",
    "\n",
    "        if model.task == 'node':\n",
    "            mask = data.val_mask if is_validation else data.test_mask\n",
    "            # node classification: only evaluate on nodes in test set\n",
    "            pred = pred[mask]\n",
    "            label = data.y[mask]\n",
    "            \n",
    "        correct += pred.eq(label).sum().item()\n",
    "     \n",
    "    if model.task == 'graph':\n",
    "        total = len(loader.dataset) \n",
    "    else:\n",
    "        total = 0\n",
    "        for data in loader.dataset:\n",
    "            total += torch.sum(data.test_mask).item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_ipython().system_raw(\n",
    "#     'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "#     .format(\"./log\")\n",
    "# )\n",
    "# get_ipython().system_raw('./ngrok http 6006 &')\n",
    "# !curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "#     \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_num = 2\n",
      "hidden_dim = 32\n",
      "the model is not learning anymore.\n",
      "Epoch 22. Loss: 1.6679. Test accuracy: 0.2083\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
    "dataset = dataset.shuffle()\n",
    "task = 'graph'\n",
    "\n",
    "model = train(dataset, task, writer, 2, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_num = 2\n",
      "hidden_dim = 32\n",
      "the model is not learning anymore.\n",
      "Epoch 24. Loss: 0.2112. Test accuracy: 0.7350\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "dataset = Planetoid(root='/tmp/cora', name='cora')\n",
    "task = 'node'\n",
    "\n",
    "model = train(dataset, task, writer, 2, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color_list = [\"red\", \"orange\", \"green\", \"blue\", \"purple\", \"brown\", \"black\"]\n",
    "\n",
    "# loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "# embs = []\n",
    "# colors = []\n",
    "# for batch in loader:\n",
    "#     emb, pred = model(batch)\n",
    "#     embs.append(emb)\n",
    "#     colors += [color_list[y] for y in batch.y]\n",
    "# embs = torch.cat(embs, dim=0)\n",
    "\n",
    "# xs, ys = zip(*TSNE().fit_transform(embs.detach().numpy()))\n",
    "# plt.scatter(xs, ys, color=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "///////////////////\n",
    "///////////////////\n",
    "///////////////////\n",
    "///////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "class Particle:\n",
    "    def __init__(self):\n",
    "        self.hidden_dim = 2\n",
    "        self.hidden_num = 2\n",
    "        # self.parameters = NN_parameters()\n",
    "        self.cognitiveCoef = 1 # can be changed\n",
    "        self.socialCoef = 1 # can be changed\n",
    "        self.informantList = list()\n",
    "        self.informants_best_err = -1\n",
    "        self.best_err = -1\n",
    "        self.best_wb = []\n",
    "        self.informants_best = [2, 2]\n",
    "        self.err = -1  # Current error (set to -1 at start\n",
    "        self.velocity_hidden_num = random.random()\n",
    "        self.velocity_hidden_dim = random.random()\n",
    "\n",
    "    def setInformants(self, swarm, informantNum, index):\n",
    "        banned_index = []\n",
    "        i = 0\n",
    "        swarm_buffer = copy.deepcopy(swarm)\n",
    "        banned_index.append(index)\n",
    "        while i < informantNum:\n",
    "            informant_chosen = np.random.randint(0, len(swarm_buffer))\n",
    "            if informant_chosen in banned_index:\n",
    "                continue\n",
    "            self.informantList.append(swarm[informant_chosen])\n",
    "            banned_index.append(informant_chosen)\n",
    "            i += 1\n",
    "\n",
    "    def set_informant_best(self):\n",
    "        for informer in self.informantList:\n",
    "            if informer.best_err < self.informants_best_err or self.informants_best_err == -1:\n",
    "                self.informants_best_err = informer.best_err\n",
    "                self.informants_best = informer.best_wb\n",
    "    \n",
    "    def check_error(self, loss):\n",
    "        self.err = 0\n",
    "        if self.err < self.best_err or self.best_err == -1:\n",
    "            self.best_err = self.err\n",
    "            self.best_wb = [self.hidden_num, self.hidden_dim]\n",
    "\n",
    "    def update_velocity(self):\n",
    "        inertia_weight = 1\n",
    "\n",
    "        r1 = random.random()\n",
    "        r2 = random.random()\n",
    "\n",
    "        vel_cog = self.cognitiveCoef * r1 * (self.best_wb[0] - self.hidden_num)\n",
    "        vel_soc = self.socialCoef * r2 * (self.informants_best[0] - self.hidden_num)\n",
    "        self.velocity_hidden_num = inertia_weight * self.velocity_hidden_num + vel_soc + vel_cog\n",
    "\n",
    "        #Change the velocity values for B1\n",
    "        r1 = random.random()\n",
    "        r2 = random.random()\n",
    "\n",
    "        vel_cog = self.cognitiveCoef * r1 * (self.best_wb[1] - self.hidden_dim)\n",
    "        vel_soc = self.socialCoef * r2 * (self.informants_best[1] - self.hidden_dim)\n",
    "        self.velocity_hidden_dim = inertia_weight * self.velocity_hidden_dim + vel_soc + vel_cog\n",
    "\n",
    "    # Update the hidden num and dim\n",
    "    def change_wb(self):\n",
    "        self.hidden_num = int(self.velocity_hidden_num + self.hidden_num)\n",
    "        self.hidden_dim = int(self.velocity_hidden_dim + self.hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swarm:\n",
    "    def __init__(self, informants_number, particle_number):\n",
    "\n",
    "        self.informants_number = informants_number\n",
    "        self.swarm = list()\n",
    "        self.particle_number = particle_number\n",
    "\n",
    "        for i in range(self.particle_number):\n",
    "            new_particle = Particle()\n",
    "            self.swarm.append(new_particle)\n",
    "        \n",
    "        for j in range(len(self.swarm)):\n",
    "            self.swarm[j].setInformants(self.swarm, self.informants_number, j)\n",
    "        \n",
    "\n",
    "    def Optimise(self):\n",
    "        #Run Optimisation\n",
    "        for p in range(0,self.particle_number):\n",
    "            # Find best informants\n",
    "            self.swarm[p].set_informant_best()\n",
    "            # Update velocities\n",
    "            self.swarm[p].update_velocity()\n",
    "            # Apply velocities to weights and biases\n",
    "            self.swarm[p].change_wb()\n",
    "\n",
    "    # For every particle, creates a neural network from the particles weights and biases. And than calculate the output values of the neural network\n",
    "    def forward_prop(self, dataset, task):\n",
    "        for p in range(0, int(self.particle_number)):\n",
    "            print(\"Particle num = \" + str(p))\n",
    "            # informants_best[0] = hidden_num\n",
    "            # informants_best[1] = hidden_dim\n",
    "            \n",
    "            # writer = SummaryWriter(\"./log/\" + \"|hidden_num = \"\n",
    "            #                         + str(self.swarm[p].informants_best[0])\n",
    "            #                         +\"|hidden_dim = \"\n",
    "            #                         + str(self.swarm[p].informants_best[1]) +\"|\")\n",
    "            writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "            # print(self.swarm[p].hidden_num)\n",
    "            # print(self.swarm[p].hidden_dim)\n",
    "            model, loss = train(dataset, task, writer, int(self.swarm[p].hidden_num), (self.swarm[p].hidden_dim))\n",
    "            self.swarm[p].check_error(loss)\n",
    "\n",
    "    # Return the best error of the entire swarn\n",
    "    def get_best(self):\n",
    "        swarm_best_err = -1\n",
    "        for p in self.swarm:\n",
    "            if p.best_err < swarm_best_err or swarm_best_err == -1:\n",
    "                swarm_best_err = p.best_err\n",
    "\n",
    "        return swarm_best_err\n",
    "\n",
    "    def plot(self, y):\n",
    "        plt.plot(y)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.ylim([0,100])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0\n",
      "Particle num = 0\n",
      "hidden_num = 2\n",
      "hidden_dim = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tristan/anaconda3/envs/thesis/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model is not learning anymore.\n",
      "Epoch 10. Loss: 1.7971. Test accuracy: 0.1083\n",
      "Particle num = 1\n",
      "hidden_num = 2\n",
      "hidden_dim = 2\n",
      "the model is not learning anymore.\n",
      "Epoch 12. Loss: 1.7987. Test accuracy: 0.1083\n",
      "Particle num = 2\n",
      "hidden_num = 2\n",
      "hidden_dim = 2\n",
      "the model is not learning anymore.\n",
      "Epoch 11. Loss: 1.7940. Test accuracy: 0.1083\n",
      "Particle num = 3\n",
      "hidden_num = 2\n",
      "hidden_dim = 2\n",
      "the model is not learning anymore.\n",
      "Epoch 12. Loss: 1.7912. Test accuracy: 0.1083\n",
      "Particle num = 4\n",
      "hidden_num = 2\n",
      "hidden_dim = 2\n",
      "the model is not learning anymore.\n",
      "Epoch 10. Loss: 1.7940. Test accuracy: 0.1083\n",
      "Particle num = 5\n",
      "hidden_num = 2\n",
      "hidden_dim = 2\n",
      "the model is not learning anymore.\n",
      "Epoch 10. Loss: 1.7920. Test accuracy: 0.1083\n",
      "Epoch = 1\n",
      "Particle num = 0\n",
      "hidden_num = 2\n",
      "hidden_dim = 2\n",
      "the model is not learning anymore.\n",
      "Epoch 12. Loss: 1.7868. Test accuracy: 0.1083\n",
      "Particle num = 1\n",
      "hidden_num = 2\n",
      "hidden_dim = 2\n",
      "the model is not learning anymore.\n",
      "Epoch 11. Loss: 1.7939. Test accuracy: 0.1083\n",
      "Particle num = 2\n",
      "hidden_num = 2\n",
      "hidden_dim = 2\n",
      "the model is not learning anymore.\n",
      "Epoch 11. Loss: 1.7973. Test accuracy: 0.1083\n",
      "Particle num = 3\n",
      "hidden_num = 2\n",
      "hidden_dim = 2\n",
      "the model is not learning anymore.\n",
      "Epoch 10. Loss: 1.7927. Test accuracy: 0.1083\n",
      "Particle num = 4\n",
      "hidden_num = 2\n",
      "hidden_dim = 2\n",
      "the model is not learning anymore.\n",
      "Epoch 10. Loss: 1.7989. Test accuracy: 0.1667\n",
      "Particle num = 5\n",
      "hidden_num = 2\n",
      "hidden_dim = 2\n",
      "the model is not learning anymore.\n",
      "Epoch 12. Loss: 1.7965. Test accuracy: 0.1667\n",
      "Epoch = 2\n",
      "Particle num = 0\n",
      "hidden_num = 2\n",
      "hidden_dim = 2\n",
      "the model is not learning anymore.\n",
      "Epoch 12. Loss: 1.7919. Test accuracy: 0.1250\n",
      "Particle num = 1\n",
      "hidden_num = 2\n",
      "hidden_dim = 2\n",
      "the model is not learning anymore.\n",
      "Epoch 12. Loss: 1.7947. Test accuracy: 0.1083\n",
      "Particle num = 2\n",
      "hidden_num = 2\n",
      "hidden_dim = 2\n",
      "the model is not learning anymore.\n",
      "Epoch 11. Loss: 1.7932. Test accuracy: 0.1667\n",
      "Particle num = 3\n",
      "hidden_num = 2\n",
      "hidden_dim = 2\n",
      "the model is not learning anymore.\n",
      "Epoch 10. Loss: 1.7898. Test accuracy: 0.1250\n",
      "Particle num = 4\n",
      "hidden_num = 2\n",
      "hidden_dim = 2\n",
      "the model is not learning anymore.\n",
      "Epoch 10. Loss: 1.7991. Test accuracy: 0.1083\n",
      "Particle num = 5\n",
      "hidden_num = 2\n",
      "hidden_dim = 2\n",
      "the model is not learning anymore.\n",
      "Epoch 13. Loss: 1.7962. Test accuracy: 0.1083\n",
      "Epoch = 3\n",
      "Particle num = 0\n",
      "hidden_num = 2\n",
      "hidden_dim = 2\n",
      "the model is not learning anymore.\n",
      "Epoch 13. Loss: 1.7949. Test accuracy: 0.1083\n",
      "Particle num = 1\n",
      "hidden_num = 2\n",
      "hidden_dim = 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000018?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000018?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch = \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000018?line=8'>9</a>\u001b[0m     swarm\u001b[39m.\u001b[39;49mforward_prop(dataset, task)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000018?line=9'>10</a>\u001b[0m     swarm\u001b[39m.\u001b[39mOptimise()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000018?line=10'>11</a>\u001b[0m     best_err \u001b[39m=\u001b[39m swarm\u001b[39m.\u001b[39mget_best()\n",
      "\u001b[1;32m/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb Cell 20\u001b[0m in \u001b[0;36mSwarm.forward_prop\u001b[0;34m(self, dataset, task)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000018?line=36'>37</a>\u001b[0m writer \u001b[39m=\u001b[39m SummaryWriter(\u001b[39m\"\u001b[39m\u001b[39m./log/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m datetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000018?line=38'>39</a>\u001b[0m \u001b[39m# print(self.swarm[p].hidden_num)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000018?line=39'>40</a>\u001b[0m \u001b[39m# print(self.swarm[p].hidden_dim)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000018?line=40'>41</a>\u001b[0m model, loss \u001b[39m=\u001b[39m train(dataset, task, writer, \u001b[39mint\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mswarm[p]\u001b[39m.\u001b[39;49mhidden_num), (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mswarm[p]\u001b[39m.\u001b[39;49mhidden_dim))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000018?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mswarm[p]\u001b[39m.\u001b[39mcheck_error(loss)\n",
      "\u001b[1;32m/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb Cell 20\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, task, writer, hidden_num, hidden_dim)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000018?line=25'>26</a>\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000018?line=26'>27</a>\u001b[0m \u001b[39m# get the prediction and the excpeted label\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000018?line=27'>28</a>\u001b[0m embedding, pred \u001b[39m=\u001b[39m model(batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000018?line=28'>29</a>\u001b[0m \u001b[39m# pred = model(batch)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000018?line=29'>30</a>\u001b[0m label \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39my\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb Cell 20\u001b[0m in \u001b[0;36mGNNStack.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000018?line=47'>48</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000018?line=48'>49</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000018?line=49'>50</a>\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlns[i](x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000018?line=51'>52</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgraph\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m# if it is a graph classification task, do a pooling\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000018?line=52'>53</a>\u001b[0m     x \u001b[39m=\u001b[39m pyg_nn\u001b[39m.\u001b[39mglobal_mean_pool(x, batch)\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.8/site-packages/torch/nn/modules/normalization.py:189\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 189\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlayer_norm(\n\u001b[1;32m    190\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.8/site-packages/torch/nn/functional.py:2486\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   2483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2484\u001b[0m         layer_norm, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, normalized_shape, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps\n\u001b[1;32m   2485\u001b[0m     )\n\u001b[0;32m-> 2486\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlayer_norm(\u001b[39minput\u001b[39;49m, normalized_shape, weight, bias, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
    "dataset = dataset.shuffle()\n",
    "task = 'graph'\n",
    "\n",
    "# 3 informants, 6 particles\n",
    "swarm = Swarm(3, 6)\n",
    "for i in range(10):\n",
    "    print(\"Epoch = \" + str(i))\n",
    "    swarm.forward_prop(dataset, task)\n",
    "    swarm.Optimise()\n",
    "    best_err = swarm.get_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "256f456f48a7573be2e92d7d4ad106f3e4ebb2f7dda185884e7558e9f4526d1c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

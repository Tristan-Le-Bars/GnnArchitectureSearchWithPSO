{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Swarm Optimisation for Graph Neural Network Architecture Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.datasets import reddit\n",
    "from torch_geometric.datasets import reddit2\n",
    "from torch_geometric.datasets import imdb\n",
    "from torch_geometric.datasets import zinc\n",
    "\n",
    "from torch_geometric.datasets import MNISTSuperpixels\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from particle import Particle\n",
    "# from swarm import Swarm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This GNN can handle different types of convolutional layers, and both node and graph classification.\n",
    "The build_conv_model method determines which type of convolutional layer to use for the given task, a graph convolutional network for node classificationtion (GCNConv) and a graph isomorphism network for graph classification (GINConv).\n",
    "This model is made of 3 covolution layers followed by mean pooling in the case of graph classification, followed by 2 fully connected layers.\n",
    "Sing our goal here is classification, we use a negative log-likelihood loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of the changeable hyperparameters:\n",
    "\n",
    "    - hidden_dim (dimmension of hidden layers)\n",
    "\n",
    "    - number of convs layers\n",
    "\n",
    "    - number of lns layers (normalisation layers)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNStack(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, task='node', hidden_num = 2, hidden_dim = 32):\n",
    "        super(GNNStack, self).__init__()\n",
    "        self.task = task\n",
    "        self.convs = nn.ModuleList() #convolution operations\n",
    "        self.lns = nn.ModuleList() #normalisation operations\n",
    "        self.convs.append(self.build_conv_model(input_dim, hidden_dim))\n",
    "        # adding convolution an normalisation layers\n",
    "        for l in range(hidden_num):\n",
    "            self.convs.append(self.build_conv_model(hidden_dim, hidden_dim))\n",
    "            # self.lns.append(nn.LayerNorm(hidden_dim))\n",
    "\n",
    "        # self.lns.append(nn.LayerNorm(hidden_dim))\n",
    "\n",
    "\n",
    "        # post-message-passing\n",
    "        self.post_mp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.Dropout(0.25), \n",
    "            nn.Linear(hidden_dim, output_dim)) # adding 2 linear layers \n",
    "        if not (self.task == 'node' or self.task == 'graph'):\n",
    "            raise RuntimeError('Unknown task.')\n",
    "\n",
    "        self.dropout = 0.25\n",
    "        self.num_layers = hidden_num\n",
    "\n",
    "    def build_conv_model(self, input_dim, hidden_dim):\n",
    "        # refer to pytorch geometric nn module for different implementation of GNNs.\n",
    "        # create different types of GCNConv according to the problem to solve\n",
    "        if self.task == 'node':\n",
    "            return pyg_nn.GCNConv(input_dim, hidden_dim)\n",
    "        else:\n",
    "            return pyg_nn.GINConv(nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
    "                                  nn.ReLU(), nn.Linear(hidden_dim, hidden_dim)))\n",
    "\n",
    "    def forward(self, data):\n",
    "        # x = feature matrix = number of nodes * number of node feature dimensions,\n",
    "        # edge_index = list of the edges in the graph,\n",
    "        # batch = batch of a graph\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        if data.num_node_features == 0: #if there is no feature, use a constant feature\n",
    "          x = torch.ones(data.num_nodes, 1)\n",
    "\n",
    "        for i in range(self.num_layers): # ,create num_layers convolution layers\n",
    "            # print(\"test\")\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            emb = x\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            # if not i == self.num_layers - 1:\n",
    "            #     x = self.lns[i](x)\n",
    "\n",
    "        if self.task == 'graph': # if it is a graph classification task, do a pooling\n",
    "            x = pyg_nn.global_mean_pool(x, batch)\n",
    "\n",
    "        x = self.post_mp(x)\n",
    "\n",
    "        return emb, F.log_softmax(x, dim=1)\n",
    "\n",
    "    def loss(self, pred, label):\n",
    "        return F.nll_loss(pred, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyg_nn.GCNConv and pyg_nn.GINConv are instances of MessagePassing, They define a single layer of graph convolution, which can be decomposed into:\n",
    "- Message computation\n",
    "- Aggregation\n",
    "- Update\n",
    "- Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model with forward propagation and back propagation. \n",
    "For node classification, we split nodes into training and testing sets.\n",
    "Same thing for graph classification, we use 80% of the graphs for training and the remainder for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, task, writer, hidden_num , hidden_dim, lr=0.01):\n",
    "    trigger = 0\n",
    "    prev_acc = 0.0\n",
    "\n",
    "    if task == 'graph':\n",
    "        data_size = len(dataset)\n",
    "        # split data  into traning and testing set\n",
    "        loader = DataLoader(dataset[:int(data_size * 0.8)], batch_size=64, shuffle=True)\n",
    "        test_loader = DataLoader(dataset[int(data_size * 0.8):], batch_size=64, shuffle=True)\n",
    "    else:\n",
    "        test_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "        loader = test_loader\n",
    "\n",
    "    # build model\n",
    "    model = GNNStack(max(dataset.num_node_features, 1), dataset.num_classes, task=task, hidden_num=hidden_num, hidden_dim=hidden_dim)\n",
    "    opt = optim.Adam(model.parameters(), lr)\n",
    "    \n",
    "    # train for 200 epochs\n",
    "    for epoch in range(200):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in loader:\n",
    "            # print(batch.train_mask, '----')\n",
    "            opt.zero_grad()\n",
    "            # get the prediction and the excpeted label\n",
    "            embedding, pred = model(batch)\n",
    "            # pred = model(batch)\n",
    "            label = batch.y\n",
    "            if task == 'node':\n",
    "                pred = pred[batch.train_mask]\n",
    "                label = label[batch.train_mask]\n",
    "            loss = model.loss(pred, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "            \n",
    "        total_loss /= len(loader.dataset)\n",
    "        writer.add_scalar(\"loss\", total_loss, epoch)\n",
    "\n",
    "        \n",
    "        test_acc = test(test_loader, model)\n",
    "        # print(\"Epoch {}. Loss: {:.4f}. Test accuracy: {:.4f}\".format(\n",
    "        #     epoch, total_loss, test_acc))\n",
    "        # writer.add_scalar(\"test accuracy\", test_acc, epoch)\n",
    "\n",
    "        if test_acc <= prev_acc:\n",
    "            trigger += 1\n",
    "\n",
    "        if trigger >= 10:\n",
    "            print(\"Epoch {}. Loss: {:.4f}. Test accuracy: {:.4f}\".format(\n",
    "            epoch, total_loss, test_acc) + \" | hidden_num = \" + str(hidden_num) + \", hidden_dim = \" + str(hidden_dim) + \", lr = \" + str(lr))\n",
    "            writer.add_scalar(\"test accuracy\", test_acc, epoch)\n",
    "            break\n",
    "        prev_acc = test_acc\n",
    "\n",
    "    return model, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the CiteSeer/Cora node classification task, there is only one graph, so we use masking to determine validation and test set. In graph classification, a subset of graphs is considered as a validation/test graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, model, is_validation=False):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        with torch.no_grad(): # avoid gradient computing for faster results\n",
    "            emb, pred = model(data)\n",
    "            pred = pred.argmax(dim=1)\n",
    "            label = data.y\n",
    "\n",
    "        if model.task == 'node':\n",
    "            mask = data.val_mask if is_validation else data.test_mask\n",
    "            # node classification: only evaluate on nodes in test set\n",
    "            pred = pred[mask]\n",
    "            label = data.y[mask]\n",
    "            \n",
    "        correct += pred.eq(label).sum().item()\n",
    "     \n",
    "    if model.task == 'graph':\n",
    "        total = len(loader.dataset) \n",
    "    else:\n",
    "        total = 0\n",
    "        for data in loader.dataset:\n",
    "            total += torch.sum(data.test_mask).item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "///////////////////\n",
    "///////////////////\n",
    "///////////////////\n",
    "///////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "class Particle:\n",
    "    def __init__(self):\n",
    "        self.hidden_dim = random.randint(2, 50)\n",
    "        self.hidden_num = random.randint(2, 10)\n",
    "        self.lr = round(random.uniform(0.0100, 0.0001), 4)\n",
    "        # self.parameters = NN_parameters()\n",
    "        self.cognitiveCoef = 1 # can be changed\n",
    "        self.socialCoef = 1 # can be changed\n",
    "        self.informantList = list()\n",
    "        self.informants_best_err = -1\n",
    "        self.best_err = -1\n",
    "        self.best_wb = []\n",
    "        self.informants_best = [self.hidden_num, self.hidden_dim, self.lr]\n",
    "        self.err = -1  # Current error (set to -1 at start\n",
    "        self.velocity_hidden_num = random.random()\n",
    "        self.velocity_hidden_dim = random.random()\n",
    "        self.velocity_lr = round(random.uniform(0.0100, 0.0001), 4)\n",
    "\n",
    "    def setInformants(self, swarm, informantNum, index):\n",
    "        banned_index = []\n",
    "        i = 0\n",
    "        swarm_buffer = copy.deepcopy(swarm)\n",
    "        banned_index.append(index)\n",
    "        while i < informantNum:\n",
    "            informant_chosen = np.random.randint(0, len(swarm_buffer))\n",
    "            if informant_chosen in banned_index:\n",
    "                continue\n",
    "            self.informantList.append(swarm[informant_chosen])\n",
    "            banned_index.append(informant_chosen)\n",
    "            i += 1\n",
    "\n",
    "    def set_informant_best(self):\n",
    "        for informer in self.informantList:\n",
    "            if informer.best_err < self.informants_best_err or self.informants_best_err == -1:\n",
    "                self.informants_best_err = informer.best_err\n",
    "                self.informants_best = informer.best_wb\n",
    "    \n",
    "    def check_error(self, loss):\n",
    "        self.err = 0\n",
    "        if self.err < self.best_err or self.best_err == -1:\n",
    "            self.best_err = self.err\n",
    "            self.best_wb = [self.hidden_num, self.hidden_dim, self.lr]\n",
    "\n",
    "    def update_velocity(self):\n",
    "        inertia_weight = 1\n",
    "\n",
    "        # Change the velocity values for hidden_num\n",
    "        r1 = random.random()\n",
    "        r2 = random.random()\n",
    "\n",
    "        vel_cog = self.cognitiveCoef * r1 * (self.best_wb[0] - self.hidden_num)\n",
    "        vel_soc = self.socialCoef * r2 * (self.informants_best[0] - self.hidden_num)\n",
    "        self.velocity_hidden_num = inertia_weight * self.velocity_hidden_num + vel_soc + vel_cog\n",
    "\n",
    "        #Change the velocity values for hidden_dim\n",
    "        r1 = random.random()\n",
    "        r2 = random.random()\n",
    "\n",
    "        vel_cog = self.cognitiveCoef * r1 * (self.best_wb[1] - self.hidden_dim)\n",
    "        vel_soc = self.socialCoef * r2 * (self.informants_best[1] - self.hidden_dim)\n",
    "        self.velocity_hidden_dim = inertia_weight * self.velocity_hidden_dim + vel_soc + vel_cog\n",
    "\n",
    "        #Change the velocity values for lr\n",
    "        r1 = random.random()\n",
    "        r2 = random.random()\n",
    "\n",
    "        vel_cog = self.cognitiveCoef * r1 * (self.best_wb[2] - self.lr)\n",
    "        vel_soc = self.socialCoef * r2 * (self.informants_best[2] - self.lr)\n",
    "        self.velocity_lr = inertia_weight * self.velocity_lr + vel_soc + vel_cog\n",
    "\n",
    "    # Update the hidden num and dim\n",
    "    def change_wb(self):\n",
    "        self.hidden_num = int(round(self.velocity_hidden_num + self.hidden_num))\n",
    "        self.hidden_dim = int(round(self.velocity_hidden_dim + self.hidden_dim))\n",
    "        self.lr = self.velocity_lr + self.lr\n",
    "        if self.hidden_num < 2:\n",
    "            self.hidden_num = 2\n",
    "        if self.hidden_dim < 2:\n",
    "            self.hidden_dim = 2\n",
    "        if self.lr <= 0:\n",
    "            self.lr = 0.0001\n",
    "        elif self.lr > 1:\n",
    "            self.lr = 1.0000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swarm:\n",
    "    def __init__(self, informants_number, particle_number):\n",
    "\n",
    "        self.informants_number = informants_number\n",
    "        self.swarm = list()\n",
    "        self.particle_number = particle_number\n",
    "\n",
    "        for i in range(self.particle_number):\n",
    "            new_particle = Particle()\n",
    "            self.swarm.append(new_particle)\n",
    "        \n",
    "        for j in range(len(self.swarm)):\n",
    "            self.swarm[j].setInformants(self.swarm, self.informants_number, j)\n",
    "        \n",
    "\n",
    "    def Optimise(self):\n",
    "        #Run Optimisation\n",
    "        for p in range(0,self.particle_number):\n",
    "            # Find best informants\n",
    "            self.swarm[p].set_informant_best()\n",
    "            # Update velocities\n",
    "            self.swarm[p].update_velocity()\n",
    "            # Apply velocities to weights and biases\n",
    "            self.swarm[p].change_wb()\n",
    "\n",
    "    # For every particle, creates a neural network from the particles weights and biases. And than calculate the output values of the neural network\n",
    "    def train_epoch(self, dataset, task):\n",
    "        for p in range(0, int(self.particle_number)):\n",
    "            print(\"Particle num = \" + str(p))\n",
    "            # informants_best[0] = hidden_num\n",
    "            # informants_best[1] = hidden_dim\n",
    "            # informants_best[2] = lr\n",
    "            writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "            model, loss = train(dataset, task, writer, int(round(self.swarm[p].hidden_num)), int(round(self.swarm[p].hidden_dim)), self.swarm[p].lr)\n",
    "            # model, loss = simple_train(dataset, task, int(round(self.swarm[p].hidden_num)), int(round(self.swarm[p].hidden_dim)), self.swarm[p].lr)\n",
    "            self.swarm[p].check_error(loss)\n",
    "\n",
    "    # Return the best error of the entire swarn\n",
    "    def get_best(self):\n",
    "        swarm_best_err = -1\n",
    "        for p in self.swarm:\n",
    "            if p.best_err < swarm_best_err or swarm_best_err == -1:\n",
    "                swarm_best_err = p.best_err\n",
    "\n",
    "        return swarm_best_err\n",
    "\n",
    "    def plot(self, y):\n",
    "        plt.plot(y)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.ylim([0,100])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "////// Epoch = 0 //////\n",
      "Particle num = 0\n",
      "Epoch 18. Loss: 1.7261. Test accuracy: 0.1833 | hidden_num = 6, hidden_dim = 32, lr = 0.0043\n",
      "Particle num = 1\n",
      "Epoch 17. Loss: 1.6710. Test accuracy: 0.2583 | hidden_num = 5, hidden_dim = 48, lr = 0.0014\n",
      "Particle num = 2\n",
      "Epoch 12. Loss: 1.7939. Test accuracy: 0.1417 | hidden_num = 7, hidden_dim = 29, lr = 0.0097\n",
      "Particle num = 3\n",
      "Epoch 21. Loss: 1.7076. Test accuracy: 0.2333 | hidden_num = 4, hidden_dim = 34, lr = 0.0057\n",
      "Particle num = 4\n",
      "Epoch 15. Loss: 1.7948. Test accuracy: 0.1417 | hidden_num = 9, hidden_dim = 10, lr = 0.0076\n",
      "Particle num = 5\n",
      "Epoch 18. Loss: 1.7390. Test accuracy: 0.1750 | hidden_num = 8, hidden_dim = 40, lr = 0.0019\n",
      "Particle num = 6\n",
      "Epoch 16. Loss: 1.7003. Test accuracy: 0.2083 | hidden_num = 4, hidden_dim = 29, lr = 0.01\n",
      "Particle num = 7\n",
      "Epoch 18. Loss: 1.7571. Test accuracy: 0.2083 | hidden_num = 4, hidden_dim = 8, lr = 0.0078\n",
      "Particle num = 8\n",
      "Epoch 17. Loss: 1.7400. Test accuracy: 0.2167 | hidden_num = 3, hidden_dim = 10, lr = 0.0091\n",
      "Particle num = 9\n",
      "Epoch 15. Loss: 1.7098. Test accuracy: 0.2167 | hidden_num = 5, hidden_dim = 28, lr = 0.0044\n",
      "Particle num = 10\n",
      "Epoch 16. Loss: 1.7300. Test accuracy: 0.2417 | hidden_num = 8, hidden_dim = 32, lr = 0.0024\n",
      "Particle num = 11\n",
      "Epoch 16. Loss: 1.7223. Test accuracy: 0.2083 | hidden_num = 4, hidden_dim = 15, lr = 0.0052\n",
      "Particle num = 12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000023?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000023?line=18'>19</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m////// Epoch = \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m //////\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000023?line=19'>20</a>\u001b[0m     swarm\u001b[39m.\u001b[39;49mtrain_epoch(dataset, task)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000023?line=20'>21</a>\u001b[0m     swarm\u001b[39m.\u001b[39mOptimise()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000023?line=21'>22</a>\u001b[0m     best_err \u001b[39m=\u001b[39m swarm\u001b[39m.\u001b[39mget_best()\n",
      "\u001b[1;32m/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb Cell 14\u001b[0m in \u001b[0;36mSwarm.train_epoch\u001b[0;34m(self, dataset, task)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000023?line=29'>30</a>\u001b[0m \u001b[39m# informants_best[0] = hidden_num\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000023?line=30'>31</a>\u001b[0m \u001b[39m# informants_best[1] = hidden_dim\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000023?line=31'>32</a>\u001b[0m \u001b[39m# informants_best[2] = lr\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000023?line=32'>33</a>\u001b[0m writer \u001b[39m=\u001b[39m SummaryWriter(\u001b[39m\"\u001b[39m\u001b[39m./log/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m datetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000023?line=34'>35</a>\u001b[0m model, loss \u001b[39m=\u001b[39m train(dataset, task, writer, \u001b[39mint\u001b[39;49m(\u001b[39mround\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mswarm[p]\u001b[39m.\u001b[39;49mhidden_num)), \u001b[39mint\u001b[39;49m(\u001b[39mround\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mswarm[p]\u001b[39m.\u001b[39;49mhidden_dim)), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mswarm[p]\u001b[39m.\u001b[39;49mlr)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000023?line=35'>36</a>\u001b[0m \u001b[39m# model, loss = simple_train(dataset, task, int(round(self.swarm[p].hidden_num)), int(round(self.swarm[p].hidden_dim)), self.swarm[p].lr)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000023?line=36'>37</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mswarm[p]\u001b[39m.\u001b[39mcheck_error(loss)\n",
      "\u001b[1;32m/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb Cell 14\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, task, writer, hidden_num, hidden_dim, lr)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000023?line=30'>31</a>\u001b[0m     label \u001b[39m=\u001b[39m label[batch\u001b[39m.\u001b[39mtrain_mask]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000023?line=31'>32</a>\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mloss(pred, label)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000023?line=32'>33</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000023?line=33'>34</a>\u001b[0m opt\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tristan/HW/GnnArchitectureSearchWithPSO/src/PsoForGnnArchitectureSearch.ipynb#ch0000023?line=34'>35</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m batch\u001b[39m.\u001b[39mnum_graphs\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
    "dataset = dataset.shuffle()\n",
    "task = 'graph'\n",
    "\n",
    "# dataset = TUDataset(root='/tmp/PROTEINS', name='PROTEINS')\n",
    "# dataset = dataset.shuffle()\n",
    "# task = 'graph'\n",
    "\n",
    "# writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "# dataset = Planetoid(root='/tmp/cora', name='cora')\n",
    "# dataset = dataset.shuffle()\n",
    "# task = 'node'\n",
    "\n",
    "# 5 informants, 50 particles\n",
    "swarm = Swarm(5, 20)\n",
    "for i in range(100):\n",
    "    print(\"////// Epoch = \" + str(i) + \" //////\")\n",
    "    swarm.train_epoch(dataset, task)\n",
    "    swarm.Optimise()\n",
    "    best_err = swarm.get_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "256f456f48a7573be2e92d7d4ad106f3e4ebb2f7dda185884e7558e9f4526d1c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
